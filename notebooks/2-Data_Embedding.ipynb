{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to embed and push the entities to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APICommander about to raise from: [{'message': \"Collection already exists: trying to create Collection ('wikidata_prototype') with different settings\", 'errorCode': 'EXISTING_COLLECTION_DIFFERENT_SETTINGS', 'id': 'c365b316-f9e7-4d01-8b4b-6f1227608760', 'family': 'REQUEST', 'title': 'Collection already exists', 'scope': 'EMPTY'}]\n",
      "/home/philippe.saade/anaconda3/lib/python3.12/site-packages/langchain_astradb/utils/astradb.py:388: UserWarning: Astra DB collection 'wikidata_prototype' is detected as having indexing turned on for all fields (either created manually or by older versions of this plugin). This implies stricter limitations on the amount of text each string in a document can store. Consider indexing anew on a fresh collection to be able to store longer texts. See https://github.com/langchain-ai/langchain-datastax/blob/main/libs/astradb/README.md#warnings-about-indexing for more details.\n",
      "  if not self._validate_indexing_policy(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'en' # Specify the language of the textified entities.\n",
    "\n",
    "from src.wikidataLangDB import create_wikidatalang_db\n",
    "from src.wikidataEmbed import WikidataTextifier\n",
    "from src.wikidataRetriever import AstraDBConnect\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from astrapy import DataAPIClient\n",
    "from sqlalchemy import text\n",
    "\n",
    "MODEL = os.getenv(\"MODEL\", \"jinaapi\")\n",
    "SAMPLE = os.getenv(\"SAMPLE\", \"false\").lower() == \"true\"\n",
    "SAMPLE_PATH = os.getenv(\"SAMPLE_PATH\", \"../data/Evaluation Data/Sample IDs (EN).pkl\")\n",
    "EMBED_BATCH_SIZE = int(os.getenv(\"EMBED_BATCH_SIZE\", 100))\n",
    "QUERY_BATCH_SIZE = int(os.getenv(\"QUERY_BATCH_SIZE\", 1000))\n",
    "OFFSET = int(os.getenv(\"OFFSET\", 0))\n",
    "API_KEY_FILENAME = os.getenv(\"API_KEY\", \"datastax_wikidata.json\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"wikidata_prototype\")\n",
    "LANGUAGE = os.getenv(\"LANGUAGE\", 'en')\n",
    "TEXTIFIER_LANGUAGE = os.getenv(\"TEXTIFIER_LANGUAGE\", None)\n",
    "DUMPDATE = os.getenv(\"DUMPDATE\", '09/18/2024')\n",
    "\n",
    "datastax_token = json.load(open(f\"../API_tokens/{API_KEY_FILENAME}\"))\n",
    "\n",
    "textifier = WikidataTextifier(language=LANGUAGE, langvar_filename=TEXTIFIER_LANGUAGE)\n",
    "graph_store = AstraDBConnect(datastax_token, COLLECTION_NAME, model=MODEL, batch_size=EMBED_BATCH_SIZE, cache_embeddings=\"wikidata_prototype\")\n",
    "\n",
    "WikidataLang = create_wikidatalang_db(db_filname=f\"sqlite_propwiki.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Wikidata entities with QIDs in a sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids = sample_ids[sample_ids['In Wikipedia']]\n",
    "total_entities = len(sample_ids)\n",
    "\n",
    "def get_entity(session):\n",
    "    sample_qids = list(sample_ids['QID'].values)[OFFSET:]\n",
    "    sample_qid_batches = [sample_qids[i:i + QUERY_BATCH_SIZE] for i in range(0, len(sample_qids), QUERY_BATCH_SIZE)]\n",
    "\n",
    "    # For each batch of sample QIDs, fetch the entities from the database\n",
    "    for qid_batch in sample_qid_batches:\n",
    "        entities = session.query(WikidataLang).filter(WikidataLang.id.in_(qid_batch)).yield_per(QUERY_BATCH_SIZE)\n",
    "        for entity in entities:\n",
    "            yield entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_entities-OFFSET) as progressbar:\n",
    "    with WikidataLang.Session() as session:\n",
    "        entity_generator = get_entity(session)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entity_generator:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, graph_store.tokenizer, max_length=graph_store.max_token_size)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                md5_hash = hashlib.md5(chunks[chunk_i].encode('utf-8')).hexdigest()\n",
    "                metadata={\n",
    "                    \"MD5\": md5_hash,\n",
    "                    \"Label\": entity.label,\n",
    "                    \"Description\": entity.description,\n",
    "                    \"Aliases\": entity.aliases,\n",
    "                    \"Date\": datetime.now().isoformat(),\n",
    "                    \"QID\": entity.id,\n",
    "                    \"ChunkID\": chunk_i+1,\n",
    "                    \"Language\": LANGUAGE,\n",
    "                    \"DumpDate\": DUMPDATE\n",
    "                }\n",
    "                graph_store.add_document(id=f\"{entity.id}_{LANGUAGE}_{chunk_i+1}\", text=chunks[chunk_i], metadata=metadata)\n",
    "\n",
    "            tqdm.write(progressbar.format_meter(progressbar.n, progressbar.total, progressbar.format_dict[\"elapsed\"])) # tqdm is not wokring in docker compose. This is the alternative\n",
    "\n",
    "        graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push all Wikidata entities found in the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entities = 9203786\n",
    "\n",
    "def get_entity(session):\n",
    "    entities = session.query(WikidataLang).offset(OFFSET).yield_per(QUERY_BATCH_SIZE)\n",
    "    for entity in entities:\n",
    "        yield entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_entities-OFFSET) as progressbar:\n",
    "    with WikidataLang.Session() as session:\n",
    "        entity_generator = get_entity(session)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entity_generator:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, graph_store.tokenizer, max_length=graph_store.max_token_size)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                md5_hash = hashlib.md5(chunks[chunk_i].encode('utf-8')).hexdigest()\n",
    "                metadata={\n",
    "                    \"MD5\": md5_hash,\n",
    "                    \"Label\": entity.label,\n",
    "                    \"Description\": entity.description,\n",
    "                    \"Aliases\": entity.aliases,\n",
    "                    \"Date\": datetime.now().isoformat(),\n",
    "                    \"QID\": entity.id,\n",
    "                    \"ChunkID\": chunk_i+1,\n",
    "                    \"Language\": LANGUAGE,\n",
    "                    \"DumpDate\": DUMPDATE\n",
    "                }\n",
    "                graph_store.add_document(id=f\"{entity.id}_{LANGUAGE}_{chunk_i+1}\", text=chunks[chunk_i], metadata=metadata)\n",
    "\n",
    "            tqdm.write(progressbar.format_meter(progressbar.n, progressbar.total, progressbar.format_dict[\"elapsed\"])) # tqdm is not wokring in docker compose. This is the alternative\n",
    "\n",
    "        graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy from one Astra collection to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata.json\"))\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_1'\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_2'\n",
    "graph_store = AstraDBConnect(datastax_token, COLLECTION_NAME, model='jina', batch_size=4, cache_embeddings=False)\n",
    "\n",
    "with tqdm(total=1347786) as progressbar:\n",
    "    for item in wikiDataCollection.find():\n",
    "        progressbar.update(1)\n",
    "        if item['metadata']['QID'] in sample_ids['QID'].values:\n",
    "            graph_store.add_document(id=item['_id'], text=item['content'], metadata=item['metadata'])\n",
    "\n",
    "    graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if all sample IDs are in Astra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata.json\"))\n",
    "COLLECTION_NAME = 'wikidata_1'\n",
    "\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids[f'in_{COLLECTION_NAME}'] = False\n",
    "\n",
    "for qid in tqdm((sample_ids[~sample_ids['in_wikidata_test_v1']]['QID'].values)):\n",
    "    item = wikiDataCollection.find_one({'metadata.QID': f'{qid}', 'metadata.Language': 'en'})\n",
    "    if item is not None:\n",
    "        sample_ids.loc[sample_ids['QID'] == qid, 'in_wikidata_test_v1'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata2.json\"))\n",
    "COLLECTION_NAME = 'wikidatav4'\n",
    "\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "ids = {}\n",
    "items = wikiDataCollection.find({'metadata.ChunkID': 2})\n",
    "for item in tqdm(items):\n",
    "    ids[item['metadata']['QID']] = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
