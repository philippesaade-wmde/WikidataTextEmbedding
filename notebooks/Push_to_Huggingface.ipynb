{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wikidataDumpReader import WikidataDumpReader\n",
    "from src.wikidataEntityDB import WikidataEntity\n",
    "from multiprocessing import Manager\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "from multiprocessing import Process, Value, Queue\n",
    "from datasets import load_dataset_builder\n",
    "import gzip\n",
    "\n",
    "FILEPATH = os.getenv(\"FILEPATH\", '../data/Wikidata/latest-all.json.bz2')\n",
    "PUSH_SIZE = int(os.getenv(\"PUSH_SIZE\", 10000))\n",
    "QUEUE_SIZE = int(os.getenv(\"QUEUE_SIZE\", 10000))\n",
    "NUM_PROCESSES = int(os.getenv(\"NUM_PROCESSES\", 4))\n",
    "SKIPLINES = 0\n",
    "\n",
    "api_key = json.load(open(\"../API_tokens/huggingface_api.json\", 'r+'))['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_items_to_sqlite(item, data_batch, sqlitDBlock):\n",
    "    if (item is not None):\n",
    "        labels = WikidataEntity.clean_label_description(item['labels'])\n",
    "        descriptions = WikidataEntity.clean_label_description(item['descriptions'])\n",
    "        labels = json.dumps(labels, separators=(',', ':'))\n",
    "        descriptions = json.dumps(descriptions, separators=(',', ':'))\n",
    "        in_wikipedia = WikidataEntity.is_in_wikipedia(item)\n",
    "        data_batch.append({\n",
    "            'id': item['id'],\n",
    "            'labels': labels,\n",
    "            'descriptions': descriptions,\n",
    "            'in_wikipedia': in_wikipedia,\n",
    "            'is_property': ('P' in item['id']),\n",
    "            'is_item': ('Q' in item['id']),\n",
    "        })\n",
    "\n",
    "        with sqlitDBlock:\n",
    "            if len(data_batch) > PUSH_SIZE:\n",
    "                worked = WikidataEntity.add_bulk_items(list(data_batch[:PUSH_SIZE]))\n",
    "                if worked:\n",
    "                    del data_batch[:PUSH_SIZE]\n",
    "\n",
    "multiprocess_manager = Manager()\n",
    "sqlitDBlock = multiprocess_manager.Lock()\n",
    "data_batch = multiprocess_manager.list()\n",
    "\n",
    "wikidata = WikidataDumpReader(FILEPATH, num_processes=NUM_PROCESSES, queue_size=QUEUE_SIZE, skiplines=SKIPLINES)\n",
    "wikidata.run(lambda item: save_items_to_sqlite(item, data_batch, sqlitDBlock), max_iterations=None, verbose=True)\n",
    "\n",
    "while len(data_batch) > 0:\n",
    "    worked = WikidataEntity.add_bulk_items(list(data_batch))\n",
    "    if worked:\n",
    "        del data_batch[:PUSH_SIZE]\n",
    "    else:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = os.getenv(\"FILEPATH\", '../data/Wikidata/latest-all.json.bz2')\n",
    "QUEUE_SIZE = int(os.getenv(\"QUEUE_SIZE\", 2000))\n",
    "NUM_PROCESSES = int(os.getenv(\"NUM_PROCESSES\", 4))\n",
    "SKIPLINES = 0\n",
    "\n",
    "def save_to_queue(item, data_batch):\n",
    "    if (item is not None):\n",
    "\n",
    "        clean_claims = WikidataEntity._remove_keys(item.get('claims', {}), ['hash', 'snaktype', 'type', 'entity-type', 'numeric-id', 'qualifiers-order', 'snaks-order'])\n",
    "        clean_claims = WikidataEntity._clean_datavalue(clean_claims)\n",
    "        clean_claims = WikidataEntity._remove_keys(clean_claims, ['id'])\n",
    "        # clean_claims = WikidataEntity._add_labels_to_claims(clean_claims)\n",
    "\n",
    "        sitelinks = WikidataEntity._remove_keys(item.get('sitelinks', {}), ['badges'])\n",
    "\n",
    "        data_batch.put({\n",
    "            'id': item['id'],\n",
    "            'labels': item['labels'],\n",
    "            'descriptions': item['descriptions'],\n",
    "            'aliases': item['aliases'],\n",
    "            'sitelinks': sitelinks,\n",
    "            'claims': clean_claims\n",
    "        })\n",
    "\n",
    "def writer_loop(data_batch, finished):\n",
    "    file_handle = None\n",
    "    NUM_ITEMS = 0\n",
    "    FILE_ID = 0\n",
    "    FILE_SIZE = 1_000_000\n",
    "\n",
    "    while True:\n",
    "        if finished.value == 1 and data_batch.empty():\n",
    "            if file_handle is not None:\n",
    "                file_handle.write('\\n]')\n",
    "                file_handle.close()\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_item = data_batch.get(timeout=1)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if next_item:\n",
    "\n",
    "            if file_handle is None:\n",
    "                file_handle = gzip.open(f'chunk_{FILE_ID}.json.gz', mode='wt')\n",
    "                file_handle.write('[\\n')\n",
    "                NUM_ITEMS = 0\n",
    "\n",
    "            json_data = json.dumps(next_item, separators=(',', ':'))\n",
    "            file_handle.write(json_data)\n",
    "            NUM_ITEMS += 1\n",
    "\n",
    "            if NUM_ITEMS >= FILE_SIZE:\n",
    "                file_handle.write('\\n]')\n",
    "                file_handle.close()\n",
    "                file_handle = None\n",
    "                FILE_ID += 1\n",
    "                NUM_ITEMS = 0\n",
    "            else:\n",
    "                file_handle.write(\",\\n\")\n",
    "\n",
    "data_batch = Queue(maxsize=QUEUE_SIZE)\n",
    "finished = Value('i', 0)\n",
    "with finished.get_lock():\n",
    "    finished.value = 0\n",
    "\n",
    "wikidata = WikidataDumpReader(FILEPATH, num_processes=NUM_PROCESSES, queue_size=QUEUE_SIZE, skiplines=SKIPLINES)\n",
    "\n",
    "writer_process = Process(\n",
    "    target=writer_loop,\n",
    "    args=(data_batch, finished)\n",
    ")\n",
    "writer_process.start()\n",
    "\n",
    "wikidata.run(lambda item: save_to_queue(item, data_batch), max_iterations=None, verbose=True)\n",
    "\n",
    "with finished.get_lock():\n",
    "    finished.value = 1\n",
    "\n",
    "writer_process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "QUEUE_SIZE = int(os.getenv(\"QUEUE_SIZE\", 5000))\n",
    "NUM_PROCESSES = int(os.getenv(\"NUM_PROCESSES\", 4))\n",
    "SKIPLINES = 0\n",
    "\n",
    "def save_to_queue(item, data_queue):\n",
    "    \"\"\"Processes and puts cleaned item into the multiprocessing queue.\"\"\"\n",
    "    if (item is not None) and (WikidataEntity.is_in_wikipedia(item)):\n",
    "        claims = WikidataEntity.add_labels_batched(item['claims'], query_batch=100)\n",
    "        data_queue.put({\n",
    "            'id': item['id'],\n",
    "            'labels': json.dumps(item['labels'], separators=(',', ':')),\n",
    "            'descriptions': json.dumps(item['descriptions'], separators=(',', ':')),\n",
    "            'aliases': json.dumps(item['aliases'], separators=(',', ':')),\n",
    "            'sitelinks': json.dumps(item['sitelinks'], separators=(',', ':')),\n",
    "            'claims': json.dumps(claims, separators=(',', ':'))\n",
    "        })\n",
    "\n",
    "def chunk_generator(filepath, num_processes=2, queue_size=5000, skip_lines=0):\n",
    "    \"\"\"\n",
    "    A generator function that reads a chunk file with WikidataDumpReader,\n",
    "    processes each item, and yields the result. It uses a multiprocessing\n",
    "    queue to handle data ingestion in parallel without storing everything\n",
    "    in memory.\n",
    "    \"\"\"\n",
    "    data_queue = Queue(maxsize=queue_size)\n",
    "    finished = Value('i', 0)\n",
    "\n",
    "    # Initialize the dump reader\n",
    "    wikidata = WikidataDumpReader(\n",
    "        filepath,\n",
    "        num_processes=num_processes,\n",
    "        queue_size=queue_size,\n",
    "        skiplines=skip_lines\n",
    "    )\n",
    "\n",
    "    # Define a function to feed items into the queue\n",
    "    def run_reader():\n",
    "        wikidata.run(lambda item: save_to_queue(item, data_queue),\n",
    "                     max_iterations=None, verbose=True)\n",
    "        with finished.get_lock():\n",
    "            finished.value = 1\n",
    "\n",
    "    # Start reader in a separate process\n",
    "    reader_proc = Process(target=run_reader)\n",
    "    reader_proc.start()\n",
    "\n",
    "    # Continuously yield items from the queue to the Dataset generator\n",
    "    while True:\n",
    "        # If reader is done AND queue is empty => stop\n",
    "        if finished.value == 1 and data_queue.empty():\n",
    "            break\n",
    "        try:\n",
    "            item = data_queue.get(timeout=1)\n",
    "        except:\n",
    "            continue\n",
    "        if item:\n",
    "            yield item\n",
    "\n",
    "    # Wait for the reader process to exit\n",
    "    reader_proc.join()\n",
    "\n",
    "# Now process each chunk file and push to the same Hugging Face repo\n",
    "HF_REPO_ID = \"wikidata\"  # Change to your actual repo on Hugging Face\n",
    "\n",
    "for i in [43, 44]:\n",
    "    login(token=api_key)\n",
    "    builder = load_dataset_builder(\"philippesaade/wikidata\")\n",
    "    print(builder.info.splits.keys())\n",
    "    if f\"chunk_{i}\" not in builder.info.splits.keys():\n",
    "        filepath = f\"../data/Wikidata/latest-all-chunks/chunk_{i}.json.gz\"\n",
    "        split_name = f\"chunk_{i}\"\n",
    "\n",
    "        print(f\"Processing {filepath} -> split={split_name}\")\n",
    "\n",
    "        # Create a Dataset from the generator\n",
    "        ds_chunk = Dataset.from_generator(lambda: chunk_generator(\n",
    "            filepath,\n",
    "            num_processes=NUM_PROCESSES,\n",
    "            queue_size=QUEUE_SIZE,\n",
    "            skip_lines=SKIPLINES\n",
    "        ))\n",
    "\n",
    "        # Push each chunk as a separate \"split\" under the same dataset repo\n",
    "        login(token=api_key)\n",
    "        ds_chunk.push_to_hub(HF_REPO_ID, split=split_name)\n",
    "        print(f\"Chunk {i} pushed to {HF_REPO_ID} as {split_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=api_key)\n",
    "ds_chunk.push_to_hub(HF_REPO_ID, split=split_name)\n",
    "print(f\"Chunk {i} pushed to {HF_REPO_ID} as {split_name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
