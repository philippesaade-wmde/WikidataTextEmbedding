{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to embed and push the entities to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'en' # Specify the language of the textified entities.\n",
    "\n",
    "from src.wikidataLangDB import create_wikidatalang_db\n",
    "from src.wikidataEmbed import WikidataTextifier\n",
    "from src.wikidataRetriever import AstraDBConnect\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from astrapy import DataAPIClient\n",
    "import pickle\n",
    "\n",
    "MODEL = os.getenv(\"MODEL\", \"jinaapi\")\n",
    "SAMPLE = os.getenv(\"SAMPLE\", \"false\").lower() == \"true\"\n",
    "SAMPLE_PATH = os.getenv(\"SAMPLE_PATH\", \"../data/Evaluation Data/Sample IDs (EN).pkl\")\n",
    "EMBED_BATCH_SIZE = int(os.getenv(\"EMBED_BATCH_SIZE\", 100))\n",
    "QUERY_BATCH_SIZE = int(os.getenv(\"QUERY_BATCH_SIZE\", 1000))\n",
    "OFFSET = int(os.getenv(\"OFFSET\", 0))\n",
    "API_KEY_FILENAME = os.getenv(\"API_KEY\", \"datastax_wikidata2.json\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"v2_09_2025\")\n",
    "LANGUAGE = os.getenv(\"LANGUAGE\", 'en')\n",
    "TEXTIFIER_LANGUAGE = os.getenv(\"TEXTIFIER_LANGUAGE\", None)\n",
    "DUMPDATE = os.getenv(\"DUMPDATE\", '09/18/2024')\n",
    "\n",
    "datastax_token = json.load(open(f\"../API_tokens/{API_KEY_FILENAME}\"))\n",
    "\n",
    "textifier = WikidataTextifier(language=LANGUAGE, langvar_filename=TEXTIFIER_LANGUAGE)\n",
    "graph_store = AstraDBConnect(datastax_token,\n",
    "                             COLLECTION_NAME,\n",
    "                             model=MODEL,\n",
    "                             batch_size=EMBED_BATCH_SIZE\n",
    "                            )\n",
    "\n",
    "WikidataLang = create_wikidatalang_db(db_filname=f\"sqlite_enwiki.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Wikidata entities with QIDs in a sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids = sample_ids[sample_ids['In Wikipedia']]\n",
    "total_entities = len(sample_ids)\n",
    "\n",
    "def get_entity(session):\n",
    "    sample_qids = list(sample_ids['QID'].values)[OFFSET:]\n",
    "    sample_qid_batches = [sample_qids[i:i + QUERY_BATCH_SIZE] for i in range(0, len(sample_qids), QUERY_BATCH_SIZE)]\n",
    "\n",
    "    # For each batch of sample QIDs, fetch the entities from the database\n",
    "    for qid_batch in sample_qid_batches:\n",
    "        entities = session.query(WikidataLang).filter(WikidataLang.id.in_(qid_batch)).yield_per(QUERY_BATCH_SIZE)\n",
    "        for entity in entities:\n",
    "            yield entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_entities-OFFSET) as progressbar:\n",
    "    with WikidataLang.Session() as session:\n",
    "        entity_generator = get_entity(session)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entity_generator:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, graph_store.tokenizer, max_length=graph_store.max_token_size)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                md5_hash = hashlib.md5(chunks[chunk_i].encode('utf-8')).hexdigest()\n",
    "                metadata={\n",
    "                    \"MD5\": md5_hash,\n",
    "                    \"Label\": entity.label,\n",
    "                    \"Description\": entity.description,\n",
    "                    \"Aliases\": entity.aliases,\n",
    "                    \"Date\": datetime.now().isoformat(),\n",
    "                    \"QID\": entity.id,\n",
    "                    \"ChunkID\": chunk_i+1,\n",
    "                    \"Language\": LANGUAGE,\n",
    "                    \"DumpDate\": DUMPDATE\n",
    "                }\n",
    "                graph_store.add_document(id=f\"{entity.id}_{LANGUAGE}_{chunk_i+1}\", text=chunks[chunk_i], metadata=metadata)\n",
    "\n",
    "            tqdm.write(progressbar.format_meter(progressbar.n, progressbar.total, progressbar.format_dict[\"elapsed\"]))\n",
    "\n",
    "        graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push all Wikidata entities found in the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entities = 9203786\n",
    "\n",
    "def get_entity(session):\n",
    "    entities = session.query(WikidataLang).filter(WikidataLang.id).offset(OFFSET).yield_per(QUERY_BATCH_SIZE)\n",
    "    for entity in entities:\n",
    "        yield entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_entities-OFFSET) as progressbar:\n",
    "    with WikidataLang.Session() as session:\n",
    "        entity_generator = get_entity(session)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entity_generator:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, graph_store.tokenizer, max_length=graph_store.max_token_size)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                md5_hash = hashlib.md5(chunks[chunk_i].encode('utf-8')).hexdigest()\n",
    "                metadata={\n",
    "                    \"MD5\": md5_hash,\n",
    "                    \"Label\": entity.label,\n",
    "                    \"Description\": entity.description,\n",
    "                    \"Aliases\": entity.aliases,\n",
    "                    \"Date\": datetime.now().isoformat(),\n",
    "                    \"QID\": entity.id,\n",
    "                    \"ChunkID\": chunk_i+1,\n",
    "                    \"Language\": LANGUAGE,\n",
    "                    \"DumpDate\": DUMPDATE\n",
    "                }\n",
    "                graph_store.add_document(id=f\"{entity.id}_{LANGUAGE}_{chunk_i+1}\", text=chunks[chunk_i], metadata=metadata)\n",
    "\n",
    "            tqdm.write(progressbar.format_meter(progressbar.n, progressbar.total, progressbar.format_dict[\"elapsed\"])) # tqdm is not wokring in docker compose. This is the alternative\n",
    "\n",
    "        graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy from one Astra collection to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata.json\"))\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_v1'\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_v2'\n",
    "graph_store = AstraDBConnect(datastax_token, COLLECTION_NAME, model='jina', batch_size=4, cache_embeddings=False)\n",
    "\n",
    "with tqdm(total=1347786) as progressbar:\n",
    "    for item in wikiDataCollection.find():\n",
    "        progressbar.update(1)\n",
    "        if item['metadata']['QID'] in sample_ids['QID'].values:\n",
    "            graph_store.add_document(id=item['_id'], text=item['content'], metadata=item['metadata'])\n",
    "\n",
    "    graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if all sample IDs are in Astra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata.json\"))\n",
    "COLLECTION_NAME = 'wikidata_v1'\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids[f'in_{COLLECTION_NAME}'] = False\n",
    "\n",
    "for item in tqdm(wikiDataCollection.find()):\n",
    "    qid = item['metadata']['QID']\n",
    "    sample_ids.loc[sample_ids['QID'] == qid, f'in_{COLLECTION_NAME}'] = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
