{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr_score(prep, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = prep.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Return the MRR\n",
    "    return ranks.apply(lambda x: 1/x[0] if len(x)>0 else 0).mean()\n",
    "\n",
    "def calculate_ndcg_score(prep, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = prep.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Calculate the DCG, the Ideal DCG and finally return the NDCG\n",
    "    dcg = ranks.apply(lambda x: sum([1/np.log2(y+1) for y in x]) if len(x)>0 else 0)\n",
    "    idcg = prep.apply(lambda x: sum([1/np.log2(y+1) for y in range(1, min(len(x[true_cols]), len(x[pred_col])) + 1)]), axis=1)\n",
    "    return (dcg/idcg).mean()\n",
    "\n",
    "def calculate_accuracy_score(df):\n",
    "    highest_score_idx = df['Retrieval Score'].apply(np.argmax)\n",
    "    top_qid = df.apply(lambda x: x['Retrieval QIDs'][highest_score_idx[x.name]], axis=1)\n",
    "    return (top_qid == df['Correct QID']).mean()\n",
    "\n",
    "def calculate_log_odds_ratio_score(df):\n",
    "    def log_odds_ratio(row):\n",
    "        correct_qid = row['Correct QID']\n",
    "        wrong_qid = row['Wrong QID']\n",
    "\n",
    "        # Find the maximum scores for the correct and wrong QIDs\n",
    "        correct_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == correct_qid]\n",
    "        wrong_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == wrong_qid]\n",
    "\n",
    "        max_correct_score = max(correct_scores, default=0.999)\n",
    "        max_wrong_score = max(wrong_scores, default=0.001)\n",
    "\n",
    "        correct_log_odds = np.log(max_correct_score / (1 - max_correct_score))\n",
    "        wrong_log_odds = np.log(max_wrong_score / (1 - max_wrong_score))\n",
    "        return correct_log_odds - wrong_log_odds\n",
    "\n",
    "    # Apply the log odds ratio calculation to each row\n",
    "    return df.apply(log_odds_ratio, axis=1).mean()\n",
    "\n",
    "def clean_results(arr_ids, arr_scores):\n",
    "    id_score_map = {}\n",
    "    for qid, score in zip(arr_ids, arr_scores):\n",
    "        qid = qid.split('_')[0]\n",
    "        id_score_map[qid] = max(id_score_map.get(qid, 0), score)\n",
    "\n",
    "    sorted_items = sorted(id_score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_ids, sorted_scores = zip(*sorted_items) if sorted_items else ([], [])\n",
    "    return list(sorted_ids), list(sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/Evaluation Data'\n",
    "for file in os.listdir(directory):\n",
    "    if ('property' in file) and ('.pkl' in file):\n",
    "        print(file)\n",
    "        filename = f\"{directory}/{file}\"\n",
    "        prep = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "        if prep['Retrieval QIDs'].apply(\n",
    "            lambda x: (x is None) or (len(x) == 0)\n",
    "        ).sum() != 0:\n",
    "            print(\"Evaluation not complete\")\n",
    "\n",
    "        prep = prep[prep['Retrieval QIDs'].apply(\n",
    "            lambda x: (x is not None) and (len(x) != 0)\n",
    "        )]\n",
    "\n",
    "        if len(prep) != 0:\n",
    "\n",
    "            prep[['Retrieval QIDs', 'Retrieval Score']] = prep.apply(\n",
    "                lambda row: pd.Series(clean_results(row['Retrieval QIDs'], row['Retrieval Score'])),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            if 'Wikidata-Disamb' in filename:\n",
    "                prep = prep[:50000]\n",
    "                print(\"Size Data:\", len(prep))\n",
    "                print(f\"Accuracy: {calculate_accuracy_score(prep)}\")\n",
    "                print(f\"Log Odds: {calculate_log_odds_ratio_score(prep)}\")\n",
    "                print()\n",
    "\n",
    "                prep = prep[prep['Correct in Wikipedia']]\n",
    "                prep = prep[prep['Wrong in Wikipedia']]\n",
    "                prep = prep[:15000]\n",
    "                print(\"Size Data:\", len(prep))\n",
    "                print(f\"Accuracy: {calculate_accuracy_score(prep)}\")\n",
    "                print(f\"Log Odds: {calculate_log_odds_ratio_score(prep)}\")\n",
    "                print()\n",
    "\n",
    "            else:\n",
    "                if 'REDFM' in filename:\n",
    "                    prep = prep[prep['Correct in Wikipedia']]\n",
    "                    prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "                else:\n",
    "                    prep = prep[prep.apply(\n",
    "                        lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']),\n",
    "                        axis=1\n",
    "                    )]\n",
    "                    # prep['Correct QIDs'] = prep.apply(\n",
    "                    #     lambda x: x['Question QIDs'] + x['Answer QIDs'],\n",
    "                    #     axis=1\n",
    "                    # )\n",
    "                    prep['Correct QIDs'] = prep['Property QIDs']\n",
    "\n",
    "                    prep = prep[prep['Correct QIDs'].apply(\n",
    "                        lambda x: len(x) > 0)\n",
    "                    ]\n",
    "\n",
    "                print(\"Size Data:\", len(prep))\n",
    "                print(f\"MRR: {calculate_mrr_score(prep, 'Retrieval QIDs', 'Correct QIDs')}\")\n",
    "                print(f\"NDCG: {calculate_ndcg_score(prep, 'Retrieval QIDs', 'Correct QIDs')}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"retrieval_results_RuBQ-wikidata_prototype-DB(en)-Query(en)_propertytest.pkl\"\n",
    "filename = f\"{directory}/{file}\"\n",
    "prep1 = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "file = \"retrieval_results_RuBQ-wikidatav11_v3_sorted_512dim-DB(en)-Query(en)_propertytest.pkl\"\n",
    "filename = f\"{directory}/{file}\"\n",
    "prep2 = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "# Define a function to merge QIDs and scores, and sort\n",
    "def merge_and_sort_qids(group):\n",
    "    # Use the first row as base\n",
    "    base = group.iloc[0].copy()\n",
    "\n",
    "    # Merge Retrieval QIDs and Scores\n",
    "    qids = []\n",
    "    scores = []\n",
    "    for q, s in zip(group[\"Retrieval QIDs\"], group[\"Retrieval Score\"]):\n",
    "        if isinstance(q, list) and isinstance(s, list):\n",
    "            qids.extend(q)\n",
    "            scores.extend(s)\n",
    "\n",
    "    # Zip and sort by score\n",
    "    sorted_pairs = sorted(zip(qids, scores), key=lambda x: -x[1])\n",
    "    sorted_qids = [qid for qid, _ in sorted_pairs]\n",
    "    sorted_scores = [score for _, score in sorted_pairs]\n",
    "\n",
    "    base[\"Retrieval QIDs\"] = sorted_qids\n",
    "    base[\"Retrieval Score\"] = sorted_scores\n",
    "    return base\n",
    "\n",
    "combined = pd.concat([prep1, prep2], ignore_index=True)\n",
    "# Group by Question and apply merge function\n",
    "merged_df = combined.groupby(\"Question\", as_index=False).apply(merge_and_sort_qids).reset_index(drop=True)\n",
    "\n",
    "\n",
    "if merged_df['Retrieval QIDs'].apply(\n",
    "    lambda x: (x is None) or (len(x) == 0)\n",
    ").sum() != 0:\n",
    "    print(\"Evaluation not complete\")\n",
    "\n",
    "merged_df = merged_df[merged_df['Retrieval QIDs'].apply(\n",
    "    lambda x: (x is not None) and (len(x) != 0)\n",
    ")]\n",
    "\n",
    "\n",
    "merged_df[['Retrieval QIDs', 'Retrieval Score']] = merged_df.apply(\n",
    "    lambda row: pd.Series(clean_results(row['Retrieval QIDs'], row['Retrieval Score'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_df = merged_df[merged_df.apply(\n",
    "    lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']),\n",
    "    axis=1\n",
    ")]\n",
    "# prep['Correct QIDs'] = prep.apply(\n",
    "#     lambda x: x['Question QIDs'] + x['Answer QIDs'],\n",
    "#     axis=1\n",
    "# )\n",
    "merged_df['Correct QIDs'] = merged_df['Property QIDs']\n",
    "\n",
    "merged_df = merged_df[merged_df['Correct QIDs'].apply(\n",
    "    lambda x: len(x) > 0)\n",
    "]\n",
    "\n",
    "print(\"Size Data:\", len(merged_df))\n",
    "print(f\"MRR: {calculate_mrr_score(merged_df, 'Retrieval QIDs', 'Correct QIDs')}\")\n",
    "print(f\"NDCG: {calculate_ndcg_score(merged_df, 'Retrieval QIDs', 'Correct QIDs')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def reciprocal_rank_fusion(listKS, listVS, K=50):\n",
    "    scores = {}\n",
    "\n",
    "    for rank, item in enumerate(listKS):\n",
    "        score = 1 / (K + rank + 1)\n",
    "        scores[item] = scores.get(item, 0) + score\n",
    "\n",
    "    for rank, item in enumerate(listVS):\n",
    "        score = 1 / (K + rank + 1)\n",
    "        scores[item] = scores.get(item, 0) + score\n",
    "\n",
    "    # Sort by descending score\n",
    "    sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [item for item, _ in sorted_items]\n",
    "\n",
    "def weighted_scores(listKS, scoresKS, listVS, scoresVS, K=50):\n",
    "    scores = {}\n",
    "\n",
    "    for item, score in zip(listKS, scoresKS):\n",
    "        z = (score - 50)/10\n",
    "        score = 1 / (1 + np.exp(-z))\n",
    "        scores[item] = scores.get(item, 0) + score/2\n",
    "\n",
    "    for item, score in zip(listVS, scoresVS):\n",
    "        scores[item] = scores.get(item, 0) + score/2\n",
    "\n",
    "    # Sort by descending score\n",
    "    sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [item for item, _ in sorted_items]\n",
    "\n",
    "\n",
    "def vectordb_query_similarity(text, QIDs):\n",
    "    \"\"\"\n",
    "    Query the Wikidata vector database and get the similarity scores between the text and a list of QIDs.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to classify.\n",
    "    - QIDs (list): List of QIDs to compare the query to.\n",
    "\n",
    "    Returns:\n",
    "    - list[dict]: A list of QIDs that are relevant to the query, sorted by similarity score.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'x-api-secret': '453d3575-8f01-4d37-bbc9-973cffbe7429'\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'query': text,\n",
    "        'qid': ','.join(QIDs)\n",
    "    }\n",
    "\n",
    "    response = requests.get(\"https://wd-vectordb.toolforge.org/similarity-score\", headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 401:\n",
    "        raise Exception('Invalid API key')\n",
    "\n",
    "    if response.status_code == 422:\n",
    "        raise Exception('Query is missing')\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "def keyword_reorder(listKS, listVS, K=50):\n",
    "    scores = {}\n",
    "    for qid in listKS:\n",
    "        if qid in listVS:\n",
    "            scores[qid] = (1/(listVS.index(qid)+1))\n",
    "        else:\n",
    "            scores[qid] = 0\n",
    "\n",
    "    sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [item for item, _ in sorted_items]\n",
    "\n",
    "def keyword_reorder_perfect(listKS, correct, K=50):\n",
    "    res = []\n",
    "    for qid in correct:\n",
    "        if qid in listKS:\n",
    "            res.append(qid)\n",
    "\n",
    "    res = res + ['Q0']*(len(listKS) - len(res))\n",
    "    return res\n",
    "\n",
    "files = [\n",
    "    '/home/philippe.saade/GitHub/WikidataTextEmbedding/data/Evaluation Data/retrieval_results_REDFM-wikidatav10_v3_sorted-DB(en)-Query(en)_wikidata_keywordsearch_bm25.pkl',\n",
    "    '/home/philippe.saade/GitHub/WikidataTextEmbedding/data/Evaluation Data/retrieval_results_REDFM-wikidatav10_v3_sorted-DB(en)-Query(en)_removeprops_sorted.pkl',\n",
    "]\n",
    "names = [\n",
    "    'Keyword Search',\n",
    "    'Sorted & Filtered + Names'\n",
    "]\n",
    "\n",
    "prep_keyword = pickle.load(open(files[0], \"rb\"))\n",
    "prep_vector = pickle.load(open(files[1], \"rb\"))\n",
    "\n",
    "prep_vector['Retrieval QIDs KS'] = prep_keyword['Retrieval QIDs']\n",
    "prep_vector['Retrieval Score KS'] = prep_keyword['Retrieval Score']\n",
    "\n",
    "prep_vector[['Retrieval QIDs', 'Retrieval Score']] = prep_vector.apply(\n",
    "    lambda row: pd.Series(clean_results(row['Retrieval QIDs'], row['Retrieval Score'])),\n",
    "    axis=1\n",
    ")\n",
    "prep_vector[['Retrieval QIDs KS', 'Retrieval Score KS']] = prep_vector.apply(\n",
    "    lambda row: pd.Series(clean_results(row['Retrieval QIDs KS'], row['Retrieval Score KS'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# prep_vector = prep_vector[prep_vector.apply(\n",
    "#     lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']),\n",
    "#     axis=1\n",
    "# )]\n",
    "# prep_vector['Correct QIDs'] = prep_vector.apply(\n",
    "#     lambda x: x['Question QIDs'] + x['Answer QIDs'],\n",
    "#     axis=1\n",
    "# )\n",
    "prep_vector = prep_vector[prep_vector['Correct in Wikipedia']]\n",
    "prep_vector['Correct QIDs'] = prep_vector['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "prep_vector = prep_vector[prep_vector['Correct QIDs'].apply(\n",
    "    lambda x: len(x) > 0)\n",
    "]\n",
    "\n",
    "sample = pickle.load(open('/home/philippe.saade/GitHub/WikidataTextEmbedding/data/Evaluation Data/Sample IDs (EN).pkl', \"rb\"))\n",
    "sample = set(sample['QID'].unique())\n",
    "\n",
    "print(\"On Full\")\n",
    "print('Vector:', calculate_mrr_score(prep_vector, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep_vector, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print('Keyword:', calculate_mrr_score(prep_vector, 'Retrieval QIDs KS', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep_vector, 'Retrieval QIDs KS', 'Correct QIDs'))\n",
    "\n",
    "prep_vector['Retrieval QIDs Fusion'] = prep_vector.apply(lambda x: reciprocal_rank_fusion(x['Retrieval QIDs KS'], x['Retrieval QIDs']), axis=1)\n",
    "\n",
    "# prep_vector['Retrieval QIDs Fusion'] = prep_vector.apply(lambda x: weighted_scores(x['Retrieval QIDs KS'], x['Retrieval Score KS'], x['Retrieval QIDs'], x['Retrieval Score']), axis=1)\n",
    "\n",
    "# prep_vector['Retrieval QIDs Fusion'] = prep_vector.apply(lambda x: keyword_reorder(x['Retrieval QIDs KS'], x['Retrieval QIDs']), axis=1)\n",
    "\n",
    "# prep_vector['Retrieval QIDs Fusion'] = prep_vector.progress_apply(lambda x: keyword_reorder(x['Retrieval QIDs'], x['Question']), axis=1)\n",
    "\n",
    "print('RRF:', calculate_mrr_score(prep_vector, 'Retrieval QIDs Fusion', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep_vector, 'Retrieval QIDs Fusion', 'Correct QIDs'))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"On Sample\")\n",
    "\n",
    "prep_vector['Retrieval QIDs KS'] = prep_vector['Retrieval QIDs KS'].apply(lambda x: [i for i in x if i in sample])\n",
    "prep_vector['Retrieval QIDs KS'] = prep_vector.apply(lambda x: x['Retrieval QIDs KS']+['Q0']*(len(x['Retrieval QIDs']) - len(x['Retrieval QIDs KS'])), axis=1)\n",
    "print('Vector:', calculate_mrr_score(prep_vector, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep_vector, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print('Keyword:', calculate_mrr_score(prep_vector, 'Retrieval QIDs KS', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep_vector, 'Retrieval QIDs KS', 'Correct QIDs'))\n",
    "\n",
    "prep_vector['Retrieval QIDs Fusion'] = prep_vector.apply(lambda x: reciprocal_rank_fusion(x['Retrieval QIDs'], x['Retrieval QIDs KS']), axis=1)\n",
    "\n",
    "print('RRF:', calculate_mrr_score(prep_vector, 'Retrieval QIDs Fusion', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep_vector, 'Retrieval QIDs Fusion', 'Correct QIDs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def rrf(row, K=20):\n",
    "    scores = {}\n",
    "\n",
    "    qids, _ = clean_results(row['Retrieval QIDs'], row['Retrieval Score'])\n",
    "    for rank, item in enumerate(qids):\n",
    "        score = 1 / (K + rank + 1)\n",
    "        scores[item] = scores.get(item, 0) + score\n",
    "\n",
    "    qids, _ = clean_results(row['Retrieval QIDs KS'], row['Retrieval Score KS'])\n",
    "    for rank, item in enumerate(qids):\n",
    "        score = 1 / (K + rank + 1.01)\n",
    "        scores[item] = scores.get(item, 0) + score\n",
    "\n",
    "    # Sort by descending score\n",
    "    sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [item for item, _ in sorted_items]\n",
    "\n",
    "def rerank_all(row):\n",
    "    qids = row['Retrieval QIDs'] + row['Retrieval QIDs KS']\n",
    "    scores = row['Retrieval Reranks'] + row['Retrieval Reranks KS']\n",
    "    qids, scores = clean_results(qids, scores)\n",
    "    return qids\n",
    "\n",
    "def rerank_vector(row):\n",
    "    qids = row['Retrieval QIDs']\n",
    "    scores = row['Retrieval Reranks']\n",
    "    qids, scores = clean_results(qids, scores)\n",
    "    return qids\n",
    "\n",
    "def rerank_keyword(row):\n",
    "    qids = row['Retrieval QIDs KS']\n",
    "    scores = row['Retrieval Reranks KS']\n",
    "    qids, scores = clean_results(qids, scores)\n",
    "    return qids\n",
    "\n",
    "def no_rerank_vector(row):\n",
    "    qids = row['Retrieval QIDs']\n",
    "    scores = row['Retrieval Score']\n",
    "    qids, scores = clean_results(qids, scores)\n",
    "    return qids\n",
    "\n",
    "def no_rerank_keyword(row):\n",
    "    qids = row['Retrieval QIDs KS']\n",
    "    scores = row['Retrieval Score KS']\n",
    "    qids, scores = clean_results(qids, scores)\n",
    "    return qids\n",
    "\n",
    "save_path = f'/home/philippe.saade/GitHub/WikidataTextEmbedding/data/Evaluation Data/retrieval_results_Mintaka-wikidata_prototype-DB(en)-Query(en)_reranked.pkl'\n",
    "prep = pickle.load(open(save_path, \"rb\"))\n",
    "\n",
    "prep = prep[prep.apply(\n",
    "    lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']),\n",
    "    axis=1\n",
    ")]\n",
    "prep['Correct QIDs'] = prep.apply(\n",
    "    lambda x: x['Question QIDs'] + x['Answer QIDs'],\n",
    "    axis=1\n",
    ")\n",
    "# prep_vector = prep_vector[prep_vector['Correct in Wikipedia']]\n",
    "# prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "# prep = prep[prep['Correct QIDs'].apply(\n",
    "#     lambda x: len(x) > 0)\n",
    "# ]\n",
    "\n",
    "prep[['Reranked QIDs']] = prep.apply(lambda row: pd.Series([no_rerank_vector(row)]), axis=1)\n",
    "print('Vector:', calculate_mrr_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "\n",
    "prep[['Reranked QIDs']] = prep.apply(lambda row: pd.Series([no_rerank_keyword(row)]), axis=1)\n",
    "print('Keyword:', calculate_mrr_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "\n",
    "prep[['Reranked QIDs']] = prep.apply(lambda row: pd.Series([rrf(row)]), axis=1)\n",
    "print('RRF:', calculate_mrr_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "print('\\t', calculate_ndcg_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "\n",
    "# prep[['Reranked QIDs']] = prep.apply(lambda row: pd.Series([rerank_vector(row)]), axis=1)\n",
    "# print('Rerank Vector:', calculate_mrr_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "# print('\\t', calculate_ndcg_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "\n",
    "# prep[['Reranked QIDs']] = prep.apply(lambda row: pd.Series([rerank_keyword(row)]), axis=1)\n",
    "# print('Rerank Keyword:', calculate_mrr_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "# print('\\t', calculate_ndcg_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "\n",
    "# prep[['Reranked QIDs']] = prep.apply(lambda row: pd.Series([rerank_all(row)]), axis=1)\n",
    "# print('Rerank All:', calculate_mrr_score(prep, 'Reranked QIDs', 'Correct QIDs'))\n",
    "# print('\\t', calculate_ndcg_score(prep, 'Reranked QIDs', 'Correct QIDs'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
