{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr_score(prep, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = prep.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Return the MRR\n",
    "    return ranks.apply(lambda x: 1/x[0] if len(x)>0 else 0).mean()\n",
    "\n",
    "def calculate_ndcg_score(prep, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = prep.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Calculate the DCG, the Ideal DCG and finally return the NDCG\n",
    "    dcg = ranks.apply(lambda x: sum([1/np.log2(y+1) for y in x]) if len(x)>0 else 0)\n",
    "    idcg = prep.apply(lambda x: sum([1/np.log2(y+1) for y in range(1, min(len(x[true_cols]), len(x[pred_col])) + 1)]), axis=1)\n",
    "    return (dcg/idcg).mean()\n",
    "\n",
    "def calculate_accuracy_score(df):\n",
    "    highest_score_idx = df['Retrieval Score'].apply(np.argmax)\n",
    "    top_qid = df.apply(lambda x: x['Retrieval QIDs'][highest_score_idx[x.name]], axis=1)\n",
    "    return (top_qid == df['Correct QID']).mean()\n",
    "\n",
    "def calculate_log_odds_ratio_score(df):\n",
    "    def log_odds_ratio(row):\n",
    "        correct_qid = row['Correct QID']\n",
    "        wrong_qid = row['Wrong QID']\n",
    "\n",
    "        # Find the maximum scores for the correct and wrong QIDs\n",
    "        correct_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == correct_qid]\n",
    "        wrong_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == wrong_qid]\n",
    "\n",
    "        max_correct_score = max(correct_scores, default=0.999)\n",
    "        max_wrong_score = max(wrong_scores, default=0.001)\n",
    "\n",
    "        correct_log_odds = np.log(max_correct_score / (1 - max_correct_score))\n",
    "        wrong_log_odds = np.log(max_wrong_score / (1 - max_wrong_score))\n",
    "        return correct_log_odds - wrong_log_odds\n",
    "\n",
    "    # Apply the log odds ratio calculation to each row\n",
    "    return df.apply(log_odds_ratio, axis=1).mean()\n",
    "\n",
    "def clean_results(arr_ids):\n",
    "    arr_ids = [qid.split('_')[0] for qid in arr_ids]\n",
    "    seen = set()\n",
    "    arr_ids = [qid for qid in arr_ids if qid not in seen and not seen.add(qid)]\n",
    "    return arr_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"/home/philippe.saade/GitHub/WikidataTextEmbedding/data/Evaluation Data/Text Results/retrieval_results_Mintaka-wikidata-DB(en)-Query(en)_allprop.pkl\"\n",
    "prep = pickle.load(open(filename, \"rb\"))\n",
    "prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"/home/philippe.saade/GitHub/WikidataTextEmbedding/data/Evaluation Data/Text Results/retrieval_results_Mintaka-wikidatav4-DB(en)-Query(en)_best.pkl\"\n",
    "prep = pickle.load(open(filename, \"rb\"))\n",
    "assert (prep['Retrieval QIDs'].apply(lambda x: (x is None) or (len(x) == 0)).sum() == 0), \"Evaluation not complete\"\n",
    "prep = prep[prep['Retrieval QIDs'].apply(lambda x: (x is not None) and (len(x) != 0))]\n",
    "prep['Retrieval QIDs'] = prep['Retrieval QIDs'].apply(lambda x: [i.split('_')[0] for i in x])\n",
    "\n",
    "# For Mintaka, LC_QuAD, and RuBQ\n",
    "prep = prep[prep.apply(lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']), axis=1)]\n",
    "prep['Retrieval Language'] = prep['Retrieval QIDs'].apply(lambda x: ['en' for i in x])\n",
    "prep['Retrieval QIDs'] = prep['Retrieval QIDs'].apply(clean_results)\n",
    "prep['Correct QIDs'] = prep.apply(lambda x: x['Question QIDs'] + x['Answer QIDs'], axis=1)\n",
    "\n",
    "# prep = prep[prep['Correct QIDs'].apply(lambda x: len(x) <= 1)]\n",
    "# For REDFM\n",
    "# prep = prep[prep['Correct in Wikipedia']]\n",
    "# prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "print(\"Size Data: \", len(prep))\n",
    "print(\"MRR:\")\n",
    "print(calculate_mrr_score(prep, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print(\"NDCG:\")\n",
    "print(calculate_ndcg_score(prep, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "\n",
    "# For Wikidata Disamb\n",
    "# print(\"Size Data:\", len(prep))\n",
    "# print(f\"Accuracy: {calculate_accuracy_score(prep)}\")\n",
    "# print(f\"Log Odds: {calculate_log_odds_ratio_score(prep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/Evaluation Data/Property Testing'\n",
    "for file in os.listdir(directory):\n",
    "    if 'Mintaka' in file:\n",
    "        print(file)\n",
    "        filename = f\"{directory}/{file}\"\n",
    "        prep = pickle.load(open(filename, \"rb\"))\n",
    "        if prep['Retrieval QIDs'].apply(lambda x: (x is None) or (len(x) == 0)).sum() != 0:\n",
    "            print(\"Evaluation not complete\")\n",
    "            # continue\n",
    "        prep = prep[prep['Retrieval QIDs'].apply(lambda x: (x is not None) and (len(x) != 0))]\n",
    "        prep['Retrieval QIDs'] = prep['Retrieval QIDs'].apply(lambda x: [i.split('_')[0] for i in x])\n",
    "\n",
    "        if 'Wikidata-Disamb' in filename:\n",
    "            print(\"Size Data:\", len(prep))\n",
    "            print(f\"Accuracy: {calculate_accuracy_score(prep)}\")\n",
    "            print(f\"Log Odds: {calculate_log_odds_ratio_score(prep)}\")\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            if 'REDFM' in filename:\n",
    "                prep = prep[prep['Correct in Wikipedia']]\n",
    "                prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "            else:\n",
    "                prep = prep[prep.apply(lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']), axis=1)]\n",
    "                prep['Correct QIDs'] = prep.apply(lambda x: x['Question QIDs'] + x['Answer QIDs'], axis=1)\n",
    "                # prep = prep[prep['Answer Type'] == 'entity']\n",
    "                prep = prep[prep['Correct QIDs'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "            print(\"Size Data:\", len(prep))\n",
    "            print(f\"MRR: {calculate_mrr_score(prep, 'Retrieval QIDs', 'Correct QIDs')}\")\n",
    "            print(f\"NDCG: {calculate_ndcg_score(prep, 'Retrieval QIDs', 'Correct QIDs')}\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
