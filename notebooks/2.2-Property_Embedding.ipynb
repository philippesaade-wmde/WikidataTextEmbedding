{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'en' # Specify the language of the textified entities.\n",
    "\n",
    "from src.wikidataEmbed import WikidataTextifier\n",
    "from src.wikidataEntityDB import WikidataProperty, Session\n",
    "from src.wikidataLangDB import create_wikidatalang_db\n",
    "from src.wikidataRetriever import AstraDBConnect\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "MODEL = os.getenv(\"MODEL\", \"jinaapi\")\n",
    "EMBED_BATCH_SIZE = int(os.getenv(\"EMBED_BATCH_SIZE\", 10))\n",
    "QUERY_BATCH_SIZE = int(os.getenv(\"QUERY_BATCH_SIZE\", 1000))\n",
    "OFFSET = int(os.getenv(\"OFFSET\", 0))\n",
    "API_KEY_FILENAME = os.getenv(\"API_KEY\", \"datastax_wikidata2.json\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"v2_09_2025\")\n",
    "LANGUAGE = os.getenv(\"LANGUAGE\", 'en')\n",
    "TEXTIFIER_LANGUAGE = os.getenv(\"TEXTIFIER_LANGUAGE\", None)\n",
    "DUMPDATE = os.getenv(\"DUMPDATE\", '09/18/2024')\n",
    "\n",
    "COLLECTION_NAME = 'v2_09_2025'\n",
    "API_KEY_FILENAME = 'datastax_wikidata2.json'\n",
    "datastax_token = json.load(open(f\"../API_tokens/{API_KEY_FILENAME}\"))\n",
    "\n",
    "textifier = WikidataTextifier(language=LANGUAGE, langvar_filename=TEXTIFIER_LANGUAGE)\n",
    "\n",
    "WikidataLang = create_wikidatalang_db(db_filname=f\"sqlite_{LANGUAGE}wiki.db\")\n",
    "graph_store = AstraDBConnect(datastax_token,\n",
    "                             COLLECTION_NAME,\n",
    "                             model=MODEL,\n",
    "                             batch_size=EMBED_BATCH_SIZE\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entities = 12711\n",
    "OFFSET = 0\n",
    "def get_property(session):\n",
    "    entities = session.query(WikidataProperty).offset(OFFSET).yield_per(QUERY_BATCH_SIZE)\n",
    "    for entity in entities:\n",
    "        yield entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_using_property(property_id, limit=100):\n",
    "    \"\"\"\n",
    "    Fetches a list of Wikidata item QIDs that link to the specified property page.\n",
    "\n",
    "    Parameters:\n",
    "        property_id (str): The Wikidata property ID (e.g., 'P31').\n",
    "        limit (int): Maximum number of items to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of QIDs that link to the property page.\n",
    "    \"\"\"\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"backlinks\",\n",
    "        \"bltitle\": f\"Property:{property_id}\",\n",
    "        \"blnamespace\": 0,\n",
    "        \"bllimit\": limit,\n",
    "        \"format\": \"json\",\n",
    "        \"srprop\": 'claims'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching backlinks:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    backlinks = data.get(\"query\", {}).get(\"backlinks\", [])\n",
    "    qids = [link[\"title\"] for link in backlinks]\n",
    "    return qids\n",
    "\n",
    "for property in tqdm(get_property(Session()), total=total_entities):\n",
    "    pid = property.id\n",
    "    examples = []\n",
    "    with WikidataLang.get_session() as session:\n",
    "        property = session.query(WikidataLang)\\\n",
    "                        .filter(WikidataLang.id==pid)\\\n",
    "                        .first()\n",
    "\n",
    "        if property:\n",
    "            qids = get_items_using_property(pid, limit=100)\n",
    "            for qid in qids:\n",
    "                subject = WikidataLang.get_entity(qid)\n",
    "                if subject and (pid in subject.claims):\n",
    "                    examples.append(subject)\n",
    "\n",
    "            text = textifier.property_to_text(property, examples)\n",
    "            item_instanceof = [c['mainsnak']['datavalue']['value']['id'] for c in property.claims.get(\"P31\", [])]\n",
    "\n",
    "            # Chunk the text if needed\n",
    "            tokens = graph_store.tokenizer(text, add_special_tokens=False, return_offsets_mapping=True)\n",
    "            token_ids, offsets = tokens['input_ids'], tokens['offset_mapping']\n",
    "            if len(token_ids) >= graph_store.max_token_size:\n",
    "                start, end = offsets[0][0], offsets[graph_store.max_token_size - 1][1]\n",
    "                text = text[start:end]\n",
    "\n",
    "            metadata = {\n",
    "                \"Label\": property.label,\n",
    "                \"Description\": property.description,\n",
    "                \"Date\": datetime.now().isoformat(),\n",
    "                \"PID\": pid,\n",
    "                \"ChunkID\": 1,\n",
    "                \"Language\": LANGUAGE,\n",
    "                \"InstanceOf\": item_instanceof,\n",
    "                \"IsItem\": pid.startswith('Q'),\n",
    "                \"IsProperty\": pid.startswith('P'),\n",
    "                \"DumpDate\": DUMPDATE\n",
    "            }\n",
    "\n",
    "            graph_store.add_document(\n",
    "                id=f\"{pid}_{LANGUAGE}_0\",\n",
    "                text=text,\n",
    "                metadata=metadata\n",
    "            )\n",
    "\n",
    "graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for property in tqdm(get_property(Session()), total=total_entities):\n",
    "    pid = property.id\n",
    "    examples = []\n",
    "    with WikidataLang.get_session() as session:\n",
    "        property = session.query(WikidataLang)\\\n",
    "                        .filter(WikidataLang.id==pid)\\\n",
    "                        .first()\n",
    "\n",
    "        if property and property.label:\n",
    "            item_instanceof = [c['mainsnak']['datavalue']['value']['id'] for c in property.claims.get(\"P31\", [])]\n",
    "\n",
    "            chunks = textifier.chunk_text(\n",
    "                property,\n",
    "                graph_store.tokenizer,\n",
    "                max_length=graph_store.max_token_size\n",
    "            )\n",
    "\n",
    "            for chunk_i, chunk in enumerate(chunks):\n",
    "                db_id = f\"{pid}_{LANGUAGE}_{chunk_i+1}\"\n",
    "                metadata = {\n",
    "                    \"Label\": property.label,\n",
    "                    \"Description\": property.description,\n",
    "                    \"Date\": datetime.now().isoformat(),\n",
    "                    \"PID\": pid,\n",
    "                    \"ChunkID\": 1,\n",
    "                    \"Language\": LANGUAGE,\n",
    "                    \"InstanceOf\": item_instanceof,\n",
    "                    \"IsItem\": pid.startswith('Q'),\n",
    "                    \"IsProperty\": pid.startswith('P'),\n",
    "                    \"DumpDate\": DUMPDATE\n",
    "                }\n",
    "\n",
    "                graph_store.add_document(\n",
    "                    id=db_id,\n",
    "                    text=chunk,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "\n",
    "graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_api(ids):\n",
    "    \"\"\"\n",
    "    Fetches a list of Wikidata item QIDs that link to the specified property page.\n",
    "\n",
    "    Parameters:\n",
    "        property_id (str): The Wikidata property ID (e.g., 'P31').\n",
    "        limit (int): Maximum number of items to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of QIDs that link to the property page.\n",
    "    \"\"\"\n",
    "    if isinstance(ids, list):\n",
    "        ids = '|'.join(set(ids))\n",
    "\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": ids,\n",
    "        \"props\": \"labels|descriptions|aliases|claims|sitelinks\",\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching backlinks:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    entities = data.get(\"entities\", {})\n",
    "    return entities\n",
    "\n",
    "pids = []\n",
    "for property in tqdm(get_property(Session()), total=total_entities):\n",
    "    pids.append(property.id)\n",
    "\n",
    "data_batch = []\n",
    "for i in tqdm(range(0, len(pids), 50)):\n",
    "    items = get_entities_api(pids[i:i+50])\n",
    "    for pid, item in items.items():\n",
    "        if 'labels' in item:\n",
    "            item = WikidataLang.normalise_item(item)\n",
    "            data_batch.append(item)\n",
    "\n",
    "WikidataLang.add_bulk_entities(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
