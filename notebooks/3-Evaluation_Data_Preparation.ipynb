{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prepare the evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'en'\n",
    "\n",
    "from sqlalchemy.sql.expression import func\n",
    "from src.wikidataLangDB import create_wikidatalang_db\n",
    "from src.wikidataEntityDB import WikidataItem\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from langdetect import detect\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "LANGUAGE = os.getenv(\"LANGUAGE\", 'en')\n",
    "WikidataLang = create_wikidatalang_db(db_filname=f\"sqlite_{LANGUAGE}wiki.db\")\n",
    "\n",
    "def is_in_wikipedia(qid):\n",
    "    item = WikidataLang.get_entity(qid)\n",
    "    return (item is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the KGConv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for _, data_chunks in data.items():\n",
    "        for data_chunk in data_chunks:\n",
    "            triple = data_chunk['triple']\n",
    "            question_in_wikipedia = is_in_wikipedia(triple[0])\n",
    "            answer_in_wikipedia = is_in_wikipedia(triple[2])\n",
    "            question = data_chunk['question variants'][0]\n",
    "            processed_rows.append({\n",
    "                'Question QID': triple[0],\n",
    "                'Property PID': triple[1],\n",
    "                'Answer QID': triple[2],\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': question['out-of-context'],\n",
    "                'Answer': data_chunk['answer']\n",
    "            })\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/KGConv/complete_version\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for folder in os.listdir(main_dir):\n",
    "        current_dir = os.path.join(main_dir, folder)\n",
    "        for file in tqdm(os.listdir(current_dir)):\n",
    "            file_path = os.path.join(current_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Mintaka dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            question_qids = [d['name'] for d in data_chunk['questionEntity'] if d['entityType'] == 'entity']\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_type = data_chunk['answer']['answerType']\n",
    "\n",
    "            answer_qids = []\n",
    "            answer_in_wikipedia = []\n",
    "            if (answer_type == 'entity') and (data_chunk['answer']['answer'] is not None):\n",
    "                answer_qids = [d['name'] for d in data_chunk['answer']['answer']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'Answer': data_chunk['answer']['mention'],\n",
    "                'Answer Type': answer_type,\n",
    "                'complexityType': data_chunk['complexityType'],\n",
    "                'Language': 'en'\n",
    "            })\n",
    "\n",
    "            for lang in data_chunk['translations'].keys():\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['translations'][lang],\n",
    "                    'Answer': data_chunk['answer']['mention'],\n",
    "                    'Answer Type': answer_type,\n",
    "                    'complexityType': data_chunk['complexityType'],\n",
    "                    'Language': lang\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/Mintaka\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        if 'json' in file:\n",
    "            file_path = os.path.join(main_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Mintaka/processed_dataframe_withcomplexity.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the RuBQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7328b5876ce4b18ac436aba9e160bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f0af66479f4d5592492af2a6613a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            if data_chunk['question_uris'] is not None:\n",
    "                question_qids = [d.split('/')[-1] for d in data_chunk['question_uris']]\n",
    "                question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "                answers = [d['wd_names']['en'][0] if len(d['wd_names']['en']) > 0 else d['label'] for d in data_chunk['answers']]\n",
    "                answer_qids = [d['value'].split('/')[-1] for d in data_chunk['answers']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "                property_qids = [d.split(\":\")[1] for d in data_chunk['question_props']]\n",
    "\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Property QIDs': property_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['question_eng'],\n",
    "                    'Answer': answers,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/RuBQ\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/RuBQ/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the LC_QuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparql(query):\n",
    "    wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "    sparql = SPARQLWrapper(wikidata_endpoint)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    answer_qids = []\n",
    "    ran_sparql = True\n",
    "    if not query.lower().strip().startswith('ask'):\n",
    "        results = []\n",
    "        retry = 5\n",
    "        ran_sparql = False\n",
    "        while retry > 0:  # Retry up to 5 times\n",
    "            try:\n",
    "                sparql.setQuery(query)\n",
    "                results = sparql.query().convert()[\"results\"][\"bindings\"]\n",
    "                retry = 0  # Exit loop if successful\n",
    "                ran_sparql = True\n",
    "            except HTTPError as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                query = re.sub(r'LIMIT \\d+', 'LIMIT 5', query, flags=re.IGNORECASE) # Include a limit if the query returns too many answers\n",
    "\n",
    "        for result in results:\n",
    "            for key in result:\n",
    "                value = result[key][\"value\"]\n",
    "                if 'www.wikidata.org' in result[key][\"value\"]:\n",
    "                    answer_qids.append(value.split('/')[-1])\n",
    "\n",
    "    return answer_qids, ran_sparql\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            matches = re.findall(r'wd:Q\\d+', data_chunk['sparql_wikidata'])\n",
    "            question_qids = [match[3:] for match in matches]\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_qids, ran_sparql = run_sparql(data_chunk['sparql_wikidata'])\n",
    "            answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            property_qids = re.findall(r'wdt:P\\d+', data_chunk['sparql_wikidata'])\n",
    "            property_qids = [match[4:] for match in property_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'SPARQL': data_chunk['sparql_wikidata'],\n",
    "                'Ran SPARQL': ran_sparql\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/LC_QuAD\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Wikidata-Disamb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        correct_in_wikipedia = is_in_wikipedia(data_chunk['correct_id'])\n",
    "        wrong_in_wikipedia = is_in_wikipedia(data_chunk['wrong_id'])\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['string'],\n",
    "            'Correct QID': data_chunk['correct_id'],\n",
    "            'Wrong QID': data_chunk['wrong_id'],\n",
    "            'Correct in Wikipedia': correct_in_wikipedia,\n",
    "            'Wrong in Wikipedia': wrong_in_wikipedia,\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/Wikidata-Disamb\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in tqdm(os.listdir(main_dir)):\n",
    "    file_path = os.path.join(main_dir, file)\n",
    "    processed_data = process_file(file_path)\n",
    "    all_data.extend(processed_data)\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the REDFM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(data):\n",
    "    processed_rows = []\n",
    "    for data_chunk in tqdm(data):\n",
    "        qid = data_chunk['uri']\n",
    "        qid_in_wikipedia = is_in_wikipedia(qid)\n",
    "        boundaries = [(e['start'], e['end']) for e in data_chunk['entities'] if e['uri'] == data_chunk['uri']]\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['title'],\n",
    "            'Entity Span': boundaries,\n",
    "            'Correct QID': qid,\n",
    "            'Correct in Wikipedia': qid_in_wikipedia,\n",
    "            'Language': data_chunk['lan']\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "huggingface_ds = load_dataset(\"Babelscape/REDFM\", \"all_languages\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "for split in huggingface_ds:\n",
    "    all_data.extend(process_file(huggingface_ds[split]))\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "\n",
    "def remove_spans(sentence, spans, replace_with='Entity'):\n",
    "    # Sort spans in ascending order to remove from left to right\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    offset = 0  # To track the shift in index after replacing each span\n",
    "\n",
    "    for start, end in spans:\n",
    "        sentence = sentence[:start - offset] + replace_with + sentence[end - offset:]\n",
    "        offset += (end - start) - len(replace_with)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "clean_data['Sentence no entity'] = clean_data.apply(lambda x: remove_spans(x['Sentence'], x['Entity Span']), axis=1)\n",
    "\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all QIDs found in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = data['Correct QID'].unique()\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Mintaka/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/RuBQ/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Question QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Answer QID'].unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Correct QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Wrong QID'].unique()])\n",
    "\n",
    "unique_ids = pd.Series(unique_ids).unique()\n",
    "unique_ids = pd.DataFrame({'QID': unique_ids})\n",
    "\n",
    "unique_ids['In Wikipedia'] = unique_ids['QID'].progress_apply(lambda x: (WikidataLang.get_entity(x) is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate questions with NLLB-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "ISO639_1_TO_NLLB = {'af': 'afr_Latn','ar': 'arb_Arab','bg': 'bul_Cyrl','bn': 'ben_Beng','ca': 'cat_Latn','cs': 'ces_Latn','cy': 'cym_Latn','da': 'dan_Latn','de': 'deu_Latn','el': 'ell_Grek','en': 'eng_Latn','es': 'spa_Latn','et': 'est_Latn','fa': 'pes_Arab','fi': 'fin_Latn','fr': 'fra_Latn','gu': 'guj_Gujr','he': 'heb_Hebr','hi': 'hin_Deva','hr': 'hrv_Latn','hu': 'hun_Latn','id': 'ind_Latn','it': 'ita_Latn','ja': 'jpn_Jpan','kn': 'kan_Knda','ko': 'kor_Hang','lt': 'lit_Latn','lv': 'lvs_Latn','mk': 'mkd_Cyrl','ml': 'mal_Mlym','mr': 'mar_Deva','ne': 'npi_Deva','nl': 'nld_Latn','no': 'nob_Latn','pa': 'pan_Guru','pl': 'pol_Latn','pt': 'por_Latn','ro': 'ron_Latn','ru': 'rus_Cyrl','sk': 'slk_Latn','sl': 'slv_Latn','so': 'som_Latn','sq': 'als_Latn','sv': 'swe_Latn','sw': 'swh_Latn','ta': 'tam_Taml','te': 'tel_Telu','th': 'tha_Thai','tl': 'tgl_Latn','tr': 'tur_Latn','uk': 'ukr_Cyrl','ur': 'urd_Arab','vi': 'vie_Latn','zh-cn': 'zho_Hans','zh-tw': 'zho_Hant',\n",
    "}\n",
    "\n",
    "def translate(text, dest_lang):\n",
    "    src_lang = ISO639_1_TO_NLLB[src_lang]\n",
    "    dest_lang = ISO639_1_TO_NLLB[dest_lang]\n",
    "\n",
    "    tokenizer.src_lang = src_lang\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(dest_lang)\n",
    "    )\n",
    "\n",
    "    translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "\n",
    "def translate_online(text, dest_lang):\n",
    "    src_lang = detect(text)\n",
    "    if src_lang == dest_lang:\n",
    "        return text\n",
    "\n",
    "    if src_lang in ['zh-cn', 'zh-tw']:\n",
    "        src_lang = 'zh'\n",
    "\n",
    "    url = f'https://cxserver.wikimedia.org/v2/translate/{src_lang}/{dest_lang}/MinT'\n",
    "    data = {\n",
    "        'html': f'<p>{text}</p>'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(url, data=data)\n",
    "        translation = r.json()['contents']\n",
    "        translation = re.sub('<[^>]*>', '', translation)\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "src_lang = 'en'\n",
    "data = pickle.load(open(f\"../data/Evaluation Data/Mintaka/processed_dataframe.pkl\", \"rb\"))\n",
    "\n",
    "if 'Translated Question' not in data.columns:\n",
    "    data['Translated Question'] = None\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for idx in tqdm(range(len(data))):\n",
    "    if pd.isna(data.at[idx, 'Translated Question']):\n",
    "        original = data.at[idx, 'Question']\n",
    "        translated = translate(original, src_lang)\n",
    "        data.at[idx, 'Translated Question'] = translated\n",
    "\n",
    "        # Save every 100 steps\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            pickle.dump(data, open(\"../data/Evaluation Data/Mintaka/processed_dataframe_translated.pkl\", \"wb\"))\n",
    "            print(f\"Checkpoint saved at row {idx}\")\n",
    "\n",
    "# Save final version\n",
    "pickle.dump(data, open(\"../data/Evaluation Data/Mintaka/processed_dataframe_translated.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
